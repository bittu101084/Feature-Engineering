{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions\n",
        "----------------\n",
        "1. What is a parameter?\n",
        "\n",
        "In feature engineering, a parameter refers to a specific value or setting that can influence how a feature is created, transformed, or used in a machine learning model. These parameters are often part of the processes or algorithms applied during feature engineering.\n",
        "\n",
        "2. What is correlation? What does negative correlation mean?\n",
        "\n",
        "Correlation measures the relationship between two variables, indicating how changes in one variable are associated with changes in another. It's expressed numerically as the correlation coefficient, typically ranging between -1 and 1.\n",
        "\n",
        "A value closer to 1 means a strong positive correlation: as one variable increases, the other increases as well.\n",
        "\n",
        "A value closer to -1 means a strong negative correlation: as one variable increases, the other decreases.\n",
        "\n",
        "A value close to 0 indicates little to no correlation between the variables.\n",
        "\n",
        "Negative correlation occurs when two variables move in opposite directions.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms and statistical models that allow computers to identify patterns in data and make decisions or predictions based on that data.\n",
        "\n",
        "Here are the key components that make up an ML system:\n",
        "\n",
        "Data:\n",
        "The foundation of ML is data. Training data (labeled or unlabeled) is used to teach the model, while test data evaluates its performance.\n",
        "\n",
        "Features:\n",
        "These are specific attributes or characteristics of the data that the model uses to make predictions. Feature engineering improves the model's accuracy.\n",
        "\n",
        "Algorithms:\n",
        "ML algorithms determine how the model learns patterns from data. Common algorithms include decision trees, support vector machines (SVM), neural networks, and more.\n",
        "\n",
        "Model:\n",
        "The model is the result of applying an algorithm to the data. It represents the learned patterns that can predict outcomes.\n",
        "\n",
        "Training Process:\n",
        "This involves feeding the model with training data, adjusting its parameters to minimize prediction errors, and improving its performance over time.\n",
        "\n",
        "Evaluation:\n",
        "The model's accuracy and performance are assessed using metrics like accuracy, precision, recall, and F1 score.\n",
        "\n",
        "Deployment:\n",
        "Once the model is trained and evaluated, it's deployed for real-world usage, such as predictions or decision-making.\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "The loss value is a critical metric in evaluating the performance of a machine learning model. It measures the difference between the model's predictions and the actual target values. Here's how it helps:\n",
        "\n",
        "Assessing Accuracy: A lower loss value generally indicates that the model's predictions are closer to the actual data, meaning the model is performing well.\n",
        "\n",
        "Guiding Optimization: During training, the model adjusts its parameters to minimize the loss value. This process helps improve the model's ability to generalize and make accurate predictions.\n",
        "\n",
        "Detecting Underfitting and Overfitting:\n",
        "\n",
        "If the loss value remains high, the model might be underfitting, meaning it's too simple to capture the complexity of the data.\n",
        "\n",
        "If the loss value is very low on the training data but high on unseen data, it may indicate overfitting, where the model memorizes the training data instead of learning general patterns.\n",
        "\n",
        "Comparing Models: When testing multiple models or approaches, loss values can serve as a benchmark to determine which model performs better on the given task.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "\n",
        "In statistics and data analysis, variables are classified based on the type of data they represent. Two common types are continuous and categorical variables. Here's a quick overview:\n",
        "\n",
        "Continuous Variables\n",
        "Represent numeric data that can take an infinite number of values within a range.\n",
        "\n",
        "These variables are often measurements, such as height, weight, temperature, or time.\n",
        "\n",
        "They can have decimals or fractions (e.g., 5.5 kg, 37.2°C).\n",
        "\n",
        "Example: The speed of a car (e.g., 60.4 km/h).\n",
        "\n",
        "Categorical Variables\n",
        "Represent data that can be divided into distinct groups or categories.\n",
        "\n",
        "These variables describe qualities or characteristics, such as gender, color, or type.\n",
        "\n",
        "They often take on a finite set of values.\n",
        "\n",
        "Nominal variables: Categories with no inherent order (e.g., car brands: Toyota, Ford, Honda).\n",
        "\n",
        "Ordinal variables: Categories with a meaningful order (e.g., ratings: poor, fair, good, excellent).\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Handling categorical variables in Machine Learning is crucial for effective model training since most algorithms work with numerical data. Here are the most common techniques:\n",
        "\n",
        "1> Encoding Methods\n",
        "Label Encoding: Each unique category is assigned a numeric label. For example, [\"Red\", \"Green\", \"Blue\"] becomes [0, 1, 2]. However, this method may introduce unintended ordinal relationships.\n",
        "\n",
        "One-Hot Encoding: Creates binary columns for each category. For example, [\"Red\", \"Green\", \"Blue\"] results in three columns, with a 1 indicating the presence of a category and 0 otherwise.\n",
        "\n",
        "Binary Encoding: Converts categories into binary digits and assigns them across columns. It's memory-efficient for datasets with many categories.\n",
        "\n",
        "Target Encoding: Encodes categories based on the mean target value in regression tasks or probability in classification tasks.\n",
        "\n",
        "> eature Hashing\n",
        "Hashing maps categories to a fixed number of numerical features using a hash function. It's often used for high-cardinality data but risks collisions (when multiple categories are mapped to the same value).\n",
        "\n",
        "> Embedding Techniques (for Deep Learning)\n",
        "Embedding maps categories into dense vector representations, capturing relationships between categories. They are particularly useful for large categorical datasets and NLP tasks.\n",
        "\n",
        "> Grouping Categories\n",
        "Sometimes, categories are grouped into fewer levels based on domain knowledge or frequency to reduce complexity and improve model performance.\n",
        "\n",
        "> Handling Missing or Rare Categories\n",
        "Replacing rare categories with \"Other\" or a common category.\n",
        "\n",
        "Imputation for missing values: Fill missing categories using the most frequent value or a placeholder.\n",
        "\n",
        "Each technique is suitable for different types of models and data. For example, One-Hot Encoding works well with algorithms like Linear Regression, while Embeddings are better suited for deep neural networks.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "In the context of Machine Learning, training and testing a dataset are essential steps to build and evaluate a model effectively:\n",
        "\n",
        "1. Training Dataset\n",
        "The training dataset is the portion of your data used to train the machine learning model.\n",
        "\n",
        "During training, the model learns patterns, relationships, and rules from the data to make predictions or classifications.\n",
        "\n",
        "This involves adjusting internal parameters (e.g., weights in a neural network) to minimize errors when predicting the target variable (output).\n",
        "\n",
        "Example: If you're teaching a model to recognize cats in images, the training dataset will include images labeled as either \"cat\" or \"not a cat.\"\n",
        "\n",
        "2. Testing Dataset\n",
        "The testing dataset is a separate portion of the data used to evaluate how well the trained model performs on new, unseen data.\n",
        "\n",
        "It checks the model’s ability to generalize and ensures it hasn’t simply memorized the training data (i.e., overfitting).\n",
        "\n",
        "Example: After training the \"cat detector,\" you test it on a fresh set of images to measure its accuracy at identifying cats.\n",
        "\n",
        "Why Separate Training and Testing?\n",
        "To avoid bias in evaluating the model’s performance.\n",
        "\n",
        "To simulate how the model will perform in real-world scenarios with new data.\n",
        "\n",
        "Testing with unseen data ensures the model is robust and reliable.\n",
        "\n",
        "A common practice is to split the dataset into train (e.g., 70-80%) and test (e.g., 20-30%) proportions. Sometimes, a validation set is also used for tuning model hyperparameters.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module within the popular Python library scikit-learn, designed to provide tools for preparing and transforming data before feeding it into a machine learning model. Proper preprocessing ensures the data is in the right format and scale, which can significantly impact the model's performance.\n",
        "\n",
        "9. What is a Test set?\n",
        "\n",
        "A Test set is a portion of the dataset used to evaluate the performance of a trained machine learning model on unseen data. It acts as a simulation of how the model will perform in real-world scenarios, where it encounters new inputs. The key purpose is to assess the model's ability to generalize, rather than memorize the training data.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "To split your dataset into training and testing sets in Python, you can use train_test_split from the sklearn.model_selection module. Here's a step-by-step guide:\n",
        "\n",
        "test_size: Specifies the proportion of the dataset used for testing (e.g., 0.2 = 20%).\n",
        "\n",
        "random_state: Ensures reproducibility by fixing the random seed.\n",
        "\n",
        "Output: You get X_train, X_test, y_train, and y_test.\n",
        "\n",
        "Here’s a structured way to approach an ML problem:\n",
        "\n",
        "Understand the Problem:\n",
        "Clearly define the problem you want to solve (e.g., classification, regression, clustering).\n",
        "\n",
        "Understand the domain and gather insights into the data’s real-world context.\n",
        "\n",
        "Data Collection and Exploration:\n",
        "\n",
        "Collect and inspect the data.\n",
        "\n",
        "Identify issues like missing values, outliers, and class imbalances.\n",
        "\n",
        "Use tools like histograms, scatter plots, and correlation matrices for initial visualization.\n",
        "\n",
        "Data Preprocessing:\n",
        "Handle missing values, outliers, and inconsistencies.\n",
        "\n",
        "Encode categorical features (e.g., One-Hot Encoding, Label Encoding).\n",
        "\n",
        "Scale numerical features (e.g., StandardScaler, MinMaxScaler).\n",
        "\n",
        "Feature Selection and Engineering:\n",
        "Select relevant features that contribute to the target variable.\n",
        "\n",
        "Generate new features from existing ones (e.g., polynomial terms, domain-specific transformations).\n",
        "\n",
        "Model Selection:\n",
        "Choose an appropriate model based on the problem type and data characteristics (e.g., Logistic Regression, Decision Trees, Neural Networks).\n",
        "\n",
        "Split the data into training, validation, and test sets.\n",
        "\n",
        "Model Training:\n",
        "Train the model using the training set.\n",
        "\n",
        "Tune hyperparameters using techniques like Grid Search or Random Search.\n",
        "\n",
        "Evaluation:\n",
        "Validate the model's performance on the test set using metrics like accuracy, precision, recall, F1 score, or RMSE.\n",
        "\n",
        "Check for issues like overfitting or underfitting.\n",
        "\n",
        "Optimization and Iteration:\n",
        "Refine the model or preprocessing steps based on evaluation results.\n",
        "\n",
        "Experiment with ensemble methods or advanced algorithms if needed.\n",
        "\n",
        "Deployment:\n",
        "Package the model and integrate it into an application or system.\n",
        "\n",
        "Monitor the model’s performance over time for real-world stability.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a critical step before fitting a machine learning model because it helps ensure that your data is clean, understood, and ready for effective model training. Here’s why EDA is so important:\n",
        "\n",
        "> Understand the Dataset\n",
        "EDA allows you to familiarize yourself with the structure of the data, including its features (columns) and their types (categorical, numerical, etc.).\n",
        "\n",
        "You can identify key distributions, trends, and relationships between features and the target variable.\n",
        "\n",
        "> Detect and Handle Issues\n",
        "Missing Values: EDA helps identify missing data and decide on imputation strategies or exclusion.\n",
        "\n",
        "Outliers: These can skew your model’s predictions, so detecting and managing them is vital.\n",
        "\n",
        "Inconsistencies: EDA uncovers anomalies or incorrect values, ensuring data integrity.\n",
        "\n",
        "> Feature Importance\n",
        "EDA helps assess how individual features contribute to the target variable and whether they should be retained, transformed, or removed.\n",
        "\n",
        "This process can reveal redundant or irrelevant features that might reduce the model’s efficiency.\n",
        "\n",
        "> Data Preprocessing\n",
        "By visualizing and summarizing data, you can decide on preprocessing techniques such as scaling, normalization, encoding categorical variables, etc.\n",
        "\n",
        "EDA ensures your data is in a format suitable for the machine learning model you plan to use.\n",
        "\n",
        "> Identify Patterns and Insights\n",
        "EDA highlights relationships between variables, such as correlations, which are useful for feature engineering or understanding the data better.\n",
        "\n",
        "Visualization techniques like scatter plots, histograms, or box plots can reveal hidden patterns.\n",
        "\n",
        "> Prevent Model Errors\n",
        "Skipping EDA risks training your model on faulty or ill-prepared data, leading to issues like overfitting, poor performance, or meaningless predictions.\n",
        "\n",
        "In essence, EDA acts as a diagnostic tool to make your dataset model-ready and to help you make informed decisions during feature selection and preprocessing. It's the bridge between raw data and effective machine learning.\n",
        "\n",
        "12. What is correlation?\n",
        "\n",
        "Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It helps quantify how one variable changes in relation to another.\n",
        "\n",
        "Key Points About Correlation:\n",
        "Range: Correlation values lie between -1 and 1:\n",
        "\n",
        "+1: Perfect positive correlation (as one variable increases, the other increases proportionally).\n",
        "\n",
        "-1: Perfect negative correlation (as one variable increases, the other decreases proportionally).\n",
        "\n",
        "0: No correlation (no relationship between the variables).\n",
        "\n",
        "Types of Correlation:\n",
        "\n",
        "Positive Correlation: Both variables move in the same direction (e.g., height and weight).\n",
        "\n",
        "Negative Correlation: Variables move in opposite directions (e.g., temperature and demand for heaters).\n",
        "\n",
        "No Correlation: Variables are independent of each other (e.g., shoe size and IQ).\n",
        "\n",
        "Correlation Coefficient:\n",
        "\n",
        "It's represented as r in statistics.\n",
        "\n",
        "Commonly computed using methods like Pearson's correlation for linear relationships or Spearman's rank correlation for non-linear relationships.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Correlation ≠ Causation: A strong correlation doesn’t imply that one variable causes the other to change.\n",
        "\n",
        "Outliers can heavily influence correlation, skewing results.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "\n",
        "Negative correlation indicates an inverse relationship between two variables. This means that as one variable increases, the other tends to decrease, and vice versa. It's expressed as a correlation coefficient between 0 and -1.\n",
        "\n",
        "Key Points About Negative Correlation:\n",
        "Closer to -1: A strong negative correlation; the variables are tightly linked in their inverse relationship.\n",
        "\n",
        "Closer to 0: A weak or negligible negative correlation; the inverse relationship is less pronounced or inconsistent.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "To find the correlation between variables in Python, you can use libraries like pandas, numpy, or even visualization libraries like matplotlib and seaborn for better insights.\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation refers to a direct relationship where one event or variable causes another event or variable to occur. It indicates a cause-and-effect relationship, meaning changes in one variable lead to changes in another.\n",
        "\n",
        "correlation shows a relationship, while causation proves cause-and-effect. Causation is harder to establish and often requires controlled experiments or rigorous statistical methods.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "An optimizer is an algorithm or method in Machine Learning and Deep Learning that updates the parameters (weights and biases) of a model to minimize the error or loss function. It determines how the model learns from data by adjusting these parameters during training.\n",
        "\n",
        "The main objective of an optimizer is to find the optimal values for the parameters that result in the lowest possible loss (or error) on the dataset. This process is crucial for improving model performance.\n",
        "\n",
        "Types of Optimizers\n",
        "Here are some common optimizers, along with their explanations and examples:\n",
        "\n",
        "> Gradient Descent\n",
        "Overview: Gradient Descent is the fundamental optimization algorithm that minimizes the loss function by moving in the direction of the steepest descent (negative gradient).\n",
        "\n",
        "Variants:\n",
        "\n",
        "Batch Gradient Descent: Uses the entire dataset to compute gradients (slow for large datasets).\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Updates parameters for each training sample, making it faster but noisier.\n",
        "\n",
        "Mini-Batch Gradient Descent: Combines the benefits of Batch and SGD by processing small batches of data.\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "\n",
        "> Momentum\n",
        "Overview: Momentum builds on SGD by accelerating convergence. It helps the model avoid oscillations by adding a fraction of the previous update to the current update.\n",
        "\n",
        "Analogy: Imagine rolling a ball down a hill—it gains momentum as it moves.\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "> AdaGrad (Adaptive Gradient Algorithm)\n",
        "Overview: Adapts the learning rate for each parameter based on the history of gradients. Parameters with larger gradients are updated less frequently.\n",
        "\n",
        "Strength: Works well for sparse data (e.g., NLP tasks).\n",
        "\n",
        "Limitation: The learning rate can decay too much over time.\n",
        "\n",
        "optimizer = Adagrad(learning_rate=0.01)\n",
        "\n",
        "> RMSProp (Root Mean Square Propagation)\n",
        "Overview: RMSProp improves AdaGrad by introducing a moving average of squared gradients, preventing learning rates from decaying too quickly.\n",
        "\n",
        "Strength: Suitable for non-stationary and online learning tasks.\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "\n",
        "> Adam (Adaptive Moment Estimation)\n",
        "Overview: Adam combines the benefits of Momentum and RMSProp. It uses both a moving average of gradients and their squared gradients to adapt the learning rate for each parameter.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Works well with sparse data.\n",
        "\n",
        "Requires little tuning of hyperparameters.\n",
        "\n",
        "Widely Used: One of the most popular optimizers in deep learning.\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "> AdaDelta\n",
        "Overview: Similar to AdaGrad but avoids the learning rate decay problem by using a moving average of gradients.\n",
        "\n",
        "Strength: Improves on AdaGrad in non-stationary settings.\n",
        "\n",
        "optimizer = Adadelta()\n",
        "\n",
        "> Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n",
        "Overview: A variant of Adam that incorporates Nesterov momentum to accelerate convergence.\n",
        "\n",
        "Strength: Works well with deep networks.\n",
        "\n",
        "optimizer = Nadam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "17. What is sklearn.linear_model?\n",
        "\n",
        "sklearn.linear_model is a module in the scikit-learn library that contains various linear models for machine learning tasks. These models are primarily used for regression and classification problems. Linear models assume that the relationship between input features and the target variable is linear, making them straightforward yet powerful tools for predictive modeling.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "model.fit() is a method used to train a model. When called, it takes input data (features) and corresponding target values (labels) to learn the patterns and relationships between them. This process involves adjusting the model's internal parameters (like weights and biases) to minimize the error or loss function, making predictions more accurate.\n",
        "\n",
        "Essentially, model.fit() teaches the model how to perform the task—be it classification, regression, or clustering—using the training data.\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "odel.predict() is a method used to make predictions using a trained machine learning model. After the model has been trained with model.fit(), predict() takes input data (features) and uses the learned patterns and relationships to generate predictions.\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "\n",
        "Continuous variables represent data that can take on an infinite number of values within a range. These are typically numerical and measured on a scale, allowing fractional or decimal values.\n",
        "\n",
        "Categorical variables represent distinct groups or categories, often qualitative in nature. These are discrete and cannot take fractional values.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Feature scaling is a preprocessing technique in Machine Learning where the values of numerical features are transformed to a standard range or scale. This ensures that all features contribute equally to the learning process, regardless of their original scale or unit of measurement.\n",
        "\n",
        "How Does It Help in Machine Learning?\n",
        "Distance-Based Algorithms:\n",
        "\n",
        "Algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), or Clustering methods calculate distances between data points. If features are on different scales, the distance calculation can be skewed.\n",
        "\n",
        "Optimization Algorithms:\n",
        "\n",
        "Methods like Gradient Descent rely on consistent scaling for smooth and efficient optimization of weights.\n",
        "\n",
        "Performance Improvement:\n",
        "\n",
        "Feature scaling often results in better model performance and accuracy by ensuring the model processes all features equally.\n",
        "\n",
        "Model Interpretability:\n",
        "\n",
        "Scaling makes coefficients or weights comparable across features in linear models, improving interpretability.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "\n",
        "Scaling in Python is typically performed using the sklearn.preprocessing module from the scikit-learn library.\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in the scikit-learn library, which provides tools for preprocessing and transforming your data to make it suitable for machine learning algorithms. Proper data preprocessing ensures the model performs effectively by addressing issues like feature scaling, encoding categorical variables, and handling missing values.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "To split data for model fitting in Python, you can use the train_test_split method from the sklearn.model_selection module. It efficiently divides your dataset into training and testing subsets for use in machine learning models.\n",
        "\n",
        "Steps to Split Data\n",
        "Prepare Your Data: Ensure your data is organized into features (X) and target (y).\n",
        "\n",
        "Import Required Libraries: Import the train_test_split function from sklearn.\n",
        "\n",
        "Split the Data: Use train_test_split to divide your data into training and testing sets. Specify the proportion for the test set (commonly 20-30%).\n",
        "\n",
        "25. Explain data encoding?\n",
        "\n",
        "Data encoding is the process of converting data into a format that machine learning algorithms can understand. In particular, it involves transforming categorical variables—those that represent distinct groups or categories—into numerical formats so they can be used in computations.\n",
        "\n",
        "Types of Data Encoding\n",
        "There are various encoding techniques, and the choice depends on the type of data and the specific problem. Here's an overview:\n",
        "\n",
        "Label Encoding:\n",
        "\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Example: [\"Red\", \"Green\", \"Blue\"] → [0, 1, 2].\n",
        "\n",
        "Pros: Simple and memory-efficient.\n",
        "\n",
        "Cons: Assumes an implicit order, which can mislead some models.\n",
        "\n",
        "One-Hot Encoding:\n",
        "\n",
        "Converts categories into binary vectors, where each category gets its own column.\n",
        "\n",
        "Example: [\"Red\", \"Green\", \"Blue\"] → [[1, 0, 0], [0, 1, 0], [0, 0, 1]].\n",
        "\n",
        "Pros: Removes ordinal relationships.\n",
        "\n",
        "Cons: Can create a large number of columns for high-cardinality data.\n",
        "\n",
        "Binary Encoding:\n",
        "\n",
        "Encodes categories into binary digits and splits them across columns.\n",
        "\n",
        "Example: [\"A\", \"B\", \"C\"] → [[0, 1], [1, 0], [1, 1]] (binary representation).\n",
        "\n",
        "Pros: Reduces dimensionality compared to One-Hot Encoding.\n",
        "\n",
        "Cons: Less interpretable.\n",
        "\n",
        "Target Encoding (for supervised learning):\n",
        "\n",
        "Replaces each category with the mean (or probability) of the target variable for that category.\n",
        "\n",
        "Example: If \"A\" → Target mean of 0.2, \"B\" → Target mean of 0.8.\n",
        "\n",
        "Pros: Captures the relationship with the target variable.\n",
        "\n",
        "Cons: Can lead to overfitting if not regularized.\n",
        "\n",
        "Frequency Encoding:\n",
        "\n",
        "Replaces each category with its frequency in the dataset.\n",
        "\n",
        "Example: If \"Dog\" appears 50 times and \"Cat\" 30 times, then [\"Dog\", \"Cat\"] → [50, 30].\n",
        "\n",
        "Hashing Encoding:\n",
        "\n",
        "Applies a hash function to categories, mapping them into a fixed number of columns.\n",
        "\n",
        "Example: [\"A\", \"B\", \"C\"] → Hash values split across a limited number of columns.\n",
        "\n",
        "Pros: Efficient for high-cardinality data.\n",
        "\n",
        "Cons: Risk of collisions where multiple categories map to the same hash.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FvbEKSyC8rlv"
      }
    }
  ]
}